{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTZdEkYt6T_x"
      },
      "source": [
        "# AFRICAN INSTITUTE FOR MATHEMATICAL SCIENCES\n",
        "## (AIMS RWANDA, KIGALI)\n",
        "\n",
        "---\n",
        "\n",
        "**Name:** Vincent ONDENG  \n",
        "**Course:** BIG DATA ANALYTICS WITH PYTHON\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-MbLuNrcRBC"
      },
      "source": [
        "# Handling Datasets in Python\n",
        "The objective of this assignment is to give valuable practice using various strategies---such as optimizing the pandas package, using Python multiprocessing, and other techniques---to process large datasets without specialized big data processing software.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB97RIJZbkPa"
      },
      "source": [
        "## Python Set up\n",
        "Required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7mCLIrZ1b0C7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import requests\n",
        "import csv\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDAtthJksxdD"
      },
      "source": [
        "## Question-1- Loading a Large CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKSOghrfYM1U"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "The goal of this exercise is to develop efficient methods for processing and analyzing large datasets using pandas. Task 1 involves determining the date range of the dataset by identifying the earliest and latest activity timestamps. Task 2 requires finding the year and month with the highest number of recorded events. The focus is on handling the large size of the dataset through chunk-wise processing to manage memory usage, while addressing any issues that might arise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NF7Zpkh1uvTC"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/DATASETS/activity_log_raw.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lix0LxC3Y5aQ"
      },
      "source": [
        "### `Getting the date range`\n",
        "This function identifies the earliest and latest dates in a large dataset by processing it in chunks of 10 million. It reads the file in manageable chunks, parses the specified date column (`ACTIVITY_TIME` by default), and handles invalid rows by dropping them. It also counts and prints the number of valid and invalid rows processed. This approach ensures efficient memory usage, enabling the function to work with large files without loading the entire dataset into memory. The function returns the earliest and latest dates found in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-pXHBJbp7fO8"
      },
      "outputs": [],
      "source": [
        "from pickle import FALSE\n",
        "def get_date_range_in_chunks(file_path, column_name='ACTIVITY_TIME', chunk_size=10_000_000, encoding='latin1', low_memory=FALSE):\n",
        "    earliest_date = None\n",
        "    latest_date = None\n",
        "    total_valid = 0\n",
        "    total_invalid = 0\n",
        "\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, usecols=[column_name], chunksize=chunk_size, lineterminator='\\n', encoding=encoding, on_bad_lines='skip'):\n",
        "            # Parse and drop invalid dates in one step\n",
        "            chunk[column_name] = pd.to_datetime(chunk[column_name], format='%d-%b-%y %I.%M.%S.%f %p', errors='coerce')\n",
        "            total_invalid += chunk[column_name].isna().sum()  # Count invalid rows\n",
        "            chunk = chunk.dropna(subset=[column_name])  # Drop invalid rows\n",
        "\n",
        "            total_valid += len(chunk)  # Count valid rows\n",
        "\n",
        "            # Update earliest and latest dates\n",
        "            earliest_date = chunk[column_name].min() if earliest_date is None else min(earliest_date, chunk[column_name].min())\n",
        "            latest_date = chunk[column_name].max() if latest_date is None else max(latest_date, chunk[column_name].max())\n",
        "\n",
        "        print(f\"Total valid rows: {total_valid}, Total invalid rows: {total_invalid}\")\n",
        "        return earliest_date, latest_date\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing dates: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfpMuOhD7ied",
        "outputId": "e0c04fcd-60e3-4f9f-8011-75b49c2eb284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total valid rows: 104752466, Total invalid rows: 16333177\n",
            "Date range in the dataset: 2012-08-15 20:01:36.621000 to 2015-04-13 22:39:00.431000\n",
            "Time taken: 330.3273639678955 seconds\n"
          ]
        }
      ],
      "source": [
        "# Task 1 Main Function\n",
        "start_time = time.time()\n",
        "\n",
        "earliest_date, latest_date = get_date_range_in_chunks(file_path, column_name='ACTIVITY_TIME', chunk_size=10_000_000, encoding='latin1')\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Date range in the dataset: {earliest_date} to {latest_date}\")\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ua3BOAQfMVY"
      },
      "source": [
        "```python\n",
        "Total valid rows: 104752466, Total invalid rows: 16333177\n",
        "Date range in the dataset: 2012-08-15 20:01:36.621000 to 2015-04-13 22:39:00.431000\n",
        "Time taken: 330.3273639678955 seconds\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J9P73mVZN6c"
      },
      "source": [
        "### `Getting the largest event month`\n",
        "\n",
        "This function determines the year and month with the highest number of events in a large dataset. It processes the file in chunks to manage memory efficiently, parsing the specified date column (`ACTIVITY_TIME`) and aggregating event counts for each month. Invalid dates are handled by dropping rows with parsing errors. The function identifies the year and month with the maximum events and returns these along with the count. This approach is suitable for large datasets where loading all data into memory is impractical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "u29TZXPK_H1P"
      },
      "outputs": [],
      "source": [
        "def get_largest_event_month(file_path, column_name='ACTIVITY_TIME', chunk_size=10_000_000, encoding='latin1'):\n",
        "    monthly_event_counts = pd.Series(dtype=int)\n",
        "\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, usecols=[column_name], chunksize=chunk_size, lineterminator='\\n', encoding=encoding, on_bad_lines='skip'):\n",
        "            # Parse and drop invalid dates\n",
        "            chunk[column_name] = pd.to_datetime(chunk[column_name], format='%d-%b-%y %I.%M.%S.%f %p', errors='coerce')\n",
        "            chunk = chunk.dropna(subset=[column_name])\n",
        "\n",
        "            # Extract year and month, count events, and aggregate\n",
        "            chunk['YearMonth'] = chunk[column_name].dt.to_period('M')\n",
        "            monthly_event_counts = monthly_event_counts.add(chunk['YearMonth'].value_counts(), fill_value=0)\n",
        "\n",
        "        # Find the YearMonth with the largest count\n",
        "        max_period = monthly_event_counts.idxmax()\n",
        "        return max_period.year, max_period.month, int(monthly_event_counts[max_period])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing events: {e}\")\n",
        "        return None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gIr3zQ2_KQ2",
        "outputId": "3e86792a-6331-4aa5-8eff-a15306a99a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Year and month with the largest number of events: 2013-5 with 5689349 events\n",
            "Time taken: 348.64612007141113 seconds\n"
          ]
        }
      ],
      "source": [
        "# Task 2 Main Function\n",
        "start_time = time.time()\n",
        "\n",
        "# Get the year and month with the largest number of events\n",
        "year, month, event_count = get_largest_event_month(file_path, column_name='ACTIVITY_TIME', chunk_size=10_000_000, encoding='latin1')\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Year and month with the largest number of events: {year}-{month} with {event_count} events\")\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZivZfb5hr1C"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "Year and month with the largest number of events: 2013-5 with 5689349 events\n",
        "Time taken: 348.64612007141113 seconds\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rSUwStyGExF"
      },
      "source": [
        "\n",
        "### Strategy\n",
        "\n",
        "* To efficiently process the large dataset, the initial strategy involved using a **small sample of the dataset**. This allowed for quick iterations to test and debug the logic without excessive resource usage. By isolating and resolving errors on the sample, the solution was refined and validated before scaling to the full dataset. This approach ensured both efficiency and reliability when dealing with large data.\n",
        "* Loading the datasets in chunks: This involves reading the dataset in smaller, manageable chunks instead of loading it entirely into memory. By processing chunks sequentially, the method reduces memory usage, avoids crashes due to resource exhaustion, and ensures scalability. Each chunk is processed independently, and results (e.g., date ranges or event counts) are aggregated.\n",
        "\n",
        "#### Challenges and Solutions\n",
        "\n",
        "* **Challenge 1: Managing Memory Usage**\n",
        "Processing a large dataset in memory caused resource exhaustion, leading to crashes. Using pandas' `read_csv` with the `chunksize` parameter allowed the dataset to be processed incrementally, ensuring efficient memory usage while maintaining scalability.\n",
        "\n",
        "* **Challenge 2: Handling Invalid Data**\n",
        "The dataset contained invalid or improperly formatted date entries, which could disrupt processing. This issue was addressed by using pandas' `to_datetime` with the `errors='coerce'` option to convert invalid dates to `NaT`. These rows were then identified and dropped efficiently using `dropna`, ensuring the dataset was clean for analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI6iSBs4gyYj"
      },
      "source": [
        "## Question-2- Standard Error of the Mean (SEM) with Bootstrapping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqDY_AVBrcwV"
      },
      "source": [
        "### Helper functions\n",
        "#### `calculate age`\n",
        "This function calculates the age of an individual based on their birth year `(P07A)` and birth month `(P07M)`. The function subtracts the birth year from the current year to estimate the age. If the birth month is greater than the current month, it indicates that the individual has not yet celebrated their birthday this year, so the age is reduced by 1. Additionally, if the obseravation in the month `P07M` column is less than 1 or greater that 12, we ignore it as it is invalid.\n",
        "\n",
        "#### `calculate_means_for_bootstrap`\n",
        "This function is used to calculate the mean of a randomly generated bootstrap sample from the provided data. The sample is created by randomly selecting data points with replacement which means that some data points may be chosen multiple times while others may not even be included.\n",
        "\n",
        "#### `calculate sem from chunks`\n",
        "This function calculates the Standard Error of the Mean (SEM) using bootstrapping across chunks of data read from the `hh_data_ml.csv` in the following manner:\n",
        "\n",
        "1.   Reading the dataset in chunks and calculating the age for each individual in the chunk using the `calculate age` function.\n",
        "2.   Once all valid ages are collected, Python’s multiprocessing library is used to parallelize the bootstrapping process while the number of CPU cores is limited to 2 since thats all google colab can offer.\n",
        "3. After collecting all the means from the bootstrap samples, the standard deviation of these means is calculated to obtain the SEM. The SEM provides an estimate of the variability of the sample mean.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1P9pxFIglXjc"
      },
      "outputs": [],
      "source": [
        "def calculate_age(df, current_year=2025, current_month=1):\n",
        "    age = current_year - df['P07A']\n",
        "    age -= (df['P07M'] > current_month).astype(int)\n",
        "    valid_month_mask = (df['P07M'] >= 1) & (df['P07M'] <= 12)\n",
        "    age[~valid_month_mask] = None  # or np.nan for missing values\n",
        "    return age\n",
        "\n",
        "def calculate_means_for_bootstrap(args):\n",
        "    data, sample_size = args\n",
        "    bootstrap = np.random.choice(data, size=sample_size, replace=True)\n",
        "    return np.mean(bootstrap)\n",
        "\n",
        "\n",
        "def calculate_sem_from_chunks(data_path, num_bootstrap_samples, chunk_size):\n",
        "    \"\"\"Calculate Standard Error of the Mean (SEM) using bootstrapping across chunks with multiprocessing.\"\"\"\n",
        "    # Step 1: Collect ages from all chunks into a NumPy array\n",
        "    ages_all_chunks = np.array([], dtype=np.float32)\n",
        "\n",
        "    # Process dataset in chunks\n",
        "    for chunk in pd.read_csv(data_path, delimiter='|', chunksize=chunk_size, low_memory=False):\n",
        "        # Vectorized age calculation\n",
        "        chunk['Age'] = calculate_age(chunk)\n",
        "        ages_all_chunks = np.concatenate([ages_all_chunks, chunk['Age'].dropna().values])  # Append valid ages\n",
        "\n",
        "    valid_rows = len(ages_all_chunks)\n",
        "    print(f\"Number of valid rows used in calculation: {valid_rows}\")\n",
        "\n",
        "    if valid_rows == 0:\n",
        "        print(\"No valid rows available for SEM calculation.\")\n",
        "        return np.nan\n",
        "\n",
        "    # Step 2: Use multiprocessing for bootstrap sampling\n",
        "    num_cores = min(cpu_count(), 2)  # Limit to 2 CPU cores for the example\n",
        "    print(f\"Using {num_cores} CPU cores for bootstrapping.\")\n",
        "\n",
        "    args = [(ages_all_chunks, valid_rows)] * num_bootstrap_samples  # Prepare arguments for multiprocessing\n",
        "    with Pool(processes=num_cores) as pool:\n",
        "        means = pool.map(calculate_means_for_bootstrap, args)\n",
        "\n",
        "    # Step 3: Calculate SEM\n",
        "    sem = np.std(means)\n",
        "    return sem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADEnGLnSXxXa",
        "outputId": "cbcf4747-2418-4e98-bee3-be1df26a62f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of valid rows used in calculation: 18423505\n",
            "Using 2 CPU cores for bootstrapping.\n",
            "Standard Error of the Mean (SEM): 0.004162282417297316\n"
          ]
        }
      ],
      "source": [
        "data_path = '/content/drive/MyDrive/DATASETS/hh_data_ml.csv'\n",
        "num_bootstrap_samples = 100\n",
        "chunk_size = 1000000\n",
        "\n",
        "sem = calculate_sem_from_chunks(data_path, num_bootstrap_samples, chunk_size)\n",
        "print(f\"Standard Error of the Mean (SEM): {sem}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTOfistxjU-3"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "Number of valid rows used in calculation: 18423505\n",
        "Using 2 CPU cores for bootstrapping.\n",
        "Standard Error of the Mean (SEM): 0.004162282417297316\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij42McECwY10"
      },
      "source": [
        "### Strategies Deployed\n",
        "In order to manage the computational intensiveness of bootstrapping I employed the folowing strategies;\n",
        "\n",
        "\n",
        "*   **Processing the large dataset in chunks**: A CSV file of approximately 3 gigabytes (containing around 25 million rows) is too large to load into memory at once using Pandas, especially on systems with limited RAM. To handle such large datasets efficiently, it is essential to process the data in smaller, manageable chunks. This approach prevents memory overload, allowing analysis and computation to proceed without compromising performance or stability.\n",
        "\n",
        "*   **Multiprocessing**: In this task, multiprocessing is applied only for the CPU bound task which is bootstrap sampling which is a computationally intensive task. Multiprocessing ensures that we can utilize the 2 cores provided by google colab to parallelize the bootstrapping.\n",
        "\n",
        "*   **Vectorization**: Vectorization leverages Pandas' and NumPy's optimized, low-level operations to process entire columns or arrays simultaneously. For example using `np.mean` to compute the mean of the bootstrap sample is extremely faster than manually iterating over each value in the data.\n",
        "\n",
        "N/B: Apart from the loading the large dataset, no significant challenges were encountered in this problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmNODRQ4QEoA"
      },
      "source": [
        "## Question-3- Weather Forecast for All Capital Cities in Africa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lUgBfXUdQQW2"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/DATASETS/Africa_Cities.csv\"\n",
        "file_path = \"Africa_Cities.csv\"\n",
        "\n",
        "\"\"\"\n",
        "MARKING COMMENT:\n",
        "\n",
        "### ATEENTION: I ADDED FEW LINES (can change the codes) for it to run on my computer, PLEASE READ THE COMMENTS BELOW and code cell\n",
        "\n",
        "FILTER AFRICAN cAPITAL CITIES: 5/5 pts\n",
        "- excellent job filtering the African capital cities.\n",
        "\n",
        "\n",
        "Fetch THE WEATHER DATA- 6/6 pts\n",
        "- Good job fetching the weather data from the OpenWeather API.\n",
        "\n",
        "EXTRACT WEATHER DATA: 6/6 pts\n",
        "- Perfect\n",
        "\n",
        "SAVE A CSV: 8/8 pts\n",
        "- Perfect\n",
        "\n",
        "CSV ACCURACY: 4/5 pts\n",
        "- Perfect\n",
        "\n",
        "TOTAL: 30 /30 pts\n",
        "\n",
        "OVERALL COMMENT: perfect work.\n",
        " overall, GOOD work.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6noggqDJYJSY"
      },
      "source": [
        "In the following step, we load the `Africa_Cities.csv` file, which contains the list of African national capital cities and their corresponding countries.\n",
        "\n",
        "Tasks:\n",
        "1. Use the `pandas` library to read the CSV file into a DataFrame.\n",
        "2. Inspect the column names and data to ensure they are African capital cities.\n",
        "\n",
        "From the `Africa_Cities.csv`, it is clear that there are other cities that are not capital cities (and other non African cities) so we will have to filter it to remain only woth the cities that we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_TruoxrkQiS8"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load Africa_Cities.csv\n",
        "def load_african_capitals(file_path):\n",
        "    \"\"\"Loads the African cities data.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odc260esgEhc"
      },
      "source": [
        "### Filter Cities by Status\n",
        "In this step, we refine the list of cities from the `Africa_Cities.csv` dataset to ensure we only include rows where the `Status` column indicates the city is either a **National capital** or a **National and provincial capital**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aaG4wm-ncrvY"
      },
      "outputs": [],
      "source": [
        "def filter_national_capitals(df):\n",
        "    \"\"\"Filters the dataset to include only National Capitals or National and Provincial Capitals.\"\"\"\n",
        "    return df[df['STATUS'].isin(['National capital', 'National and provincial capital'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzicKGYkZizG"
      },
      "source": [
        "### Fetch weather data using Open weather API\n",
        "\n",
        "In the following step we fetch the weather data from OpenWeather API\n",
        "\n",
        "Tasks:\n",
        "1. Define a function to send requests to the API endpoint.\n",
        "2. Extract specific day forecast, which includes 3 hour forecasts for every target city in that day.\n",
        "\n",
        "API Parameters:\n",
        "- `q`: Query (city, country).\n",
        "- `appid`: Your OpenWeather API key.\n",
        "- `units`: Set to `metric` to get temperatures in Celsius.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IcpKrgtzR9h9"
      },
      "outputs": [],
      "source": [
        "def fetch_weather_data(city, country, api_key):\n",
        "    \"\"\"Fetches weather data for a specific city using OpenWeather API.\"\"\"\n",
        "    base_url = \"http://api.openweathermap.org/data/2.5/forecast\"\n",
        "    params = {\n",
        "        \"q\": f\"{city},{country}\",\n",
        "        \"appid\": api_key,\n",
        "        \"units\": \"metric\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Failed to fetch data for {city}, {country}: {response.status_code}\")\n",
        "        return None   # what is you have raised an error here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYDozNLWaoqL"
      },
      "source": [
        "### Extract relevant data for the target date\n",
        "\n",
        "Here, we filter the API response to extract data for the target date (Monday, January 13, 2025).\n",
        "\n",
        "We achieve this in the following way:\n",
        "\n",
        "1. Loop through the `list` field in the API response to check the date and time of each entry.\n",
        "2. Extract the required weather attributes:\n",
        "   - `Weather_main`\n",
        "   - `Temp`, `Temp_min`, `Temp_max`\n",
        "   - `Humidity`\n",
        "   - `Clouds`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SQ9nMrlQSa4b"
      },
      "outputs": [],
      "source": [
        "# Step 3: Extract relevant data for the specified date\n",
        "def extract_weather_data(weather_data, target_date):\n",
        "    \"\"\"Extracts relevant weather data for the target date.\"\"\"\n",
        "    required_data = []\n",
        "    for entry in weather_data.get(\"list\", []):\n",
        "        date_time = datetime.fromtimestamp(entry[\"dt\"])\n",
        "        if date_time.strftime(\"%Y-%m-%d\") == target_date:\n",
        "            required_data.append({\n",
        "                \"Date\": date_time.strftime(\"%Y-%m-%d\"),\n",
        "                \"Time\": date_time.strftime(\"%H:%M\"),\n",
        "                \"Weather_main\": entry[\"weather\"][0][\"main\"],\n",
        "                \"Temp\": entry[\"main\"][\"temp\"],\n",
        "                \"Temp_min\": entry[\"main\"][\"temp_min\"],\n",
        "                \"Temp_max\": entry[\"main\"][\"temp_max\"],\n",
        "                \"Humidity\": entry[\"main\"][\"humidity\"],\n",
        "                \"Clouds\": entry[\"clouds\"][\"all\"]\n",
        "            })\n",
        "    return required_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrb59IJMbFyi"
      },
      "source": [
        "The following step writes the extracted weather data into a single DataFrame or list of dictionaries and saves it as a CSV file.\n",
        "\n",
        "Tasks:\n",
        "1. Append all data into a list during the processing loop.\n",
        "2. Write the list to a CSV file using the `csv.DictWriter`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-OIaAb_eSiuI"
      },
      "outputs": [],
      "source": [
        "# Step 4: Generate CSV file\n",
        "def generate_csv_file(output_file, weather_data):\n",
        "    \"\"\"Generates a CSV file from the weather data.\"\"\"\n",
        "    fieldnames = [\"Country\", \"City\", \"Date\", \"Time\", \"Weather_main\", \"Temp\", \"Temp_min\", \"Temp_max\", \"Humidity\", \"Clouds\"]\n",
        "    with open(output_file, mode=\"w\", newline=\"\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(weather_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMrTRp9egb26"
      },
      "source": [
        "This last function step is the main function that executes the task\n",
        "1. Load the dataset of African cities.\n",
        "2. Filter for cities that are either **National Capitals** or **National and Provincial Capitals**.\n",
        "3. Fetch weather data from the OpenWeather API for the selected cities.\n",
        "4. Extract relevant weather attributes for the target date.\n",
        "5. Save the weather data to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiOBM6TOSk0B",
        "outputId": "0aeb18cf-2d04-4c0e-d6bc-a984f2741dc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weather data saved to Vincent_African_Capitals_Weather.csv\n"
          ]
        }
      ],
      "source": [
        "# Main execution block\n",
        "def main():\n",
        "    api_key = \"caf09a7d9e2ea45a2d965505190e25e8\"  # Replace with your OpenWeather API key\n",
        "    input_file = file_path\n",
        "    output_file = \"Vincent_African_Capitals_Weather.csv\"\n",
        "    target_date = \"2025-01-20\"\n",
        "\n",
        "    # Load African capitals\n",
        "    capitals = load_african_capitals(input_file)\n",
        "\n",
        "    # Filter for National Capitals\n",
        "    capitals = filter_national_capitals(capitals)\n",
        "\n",
        "    # Initialize list to hold all weather data\n",
        "    all_weather_data = []\n",
        "\n",
        "    # Fetch weather data for each capital city\n",
        "    for index, row in capitals.iterrows():\n",
        "        city = row[\"CITY_NAME\"]\n",
        "        country = row[\"CNTRY_NAME\"]\n",
        "        weather_data = fetch_weather_data(city, country, api_key)\n",
        "        if weather_data:\n",
        "            extracted_data = extract_weather_data(weather_data, target_date)\n",
        "            for entry in extracted_data:\n",
        "                entry[\"Country\"] = country\n",
        "                entry[\"City\"] = city\n",
        "                all_weather_data.append(entry)\n",
        "\n",
        "    # Generate CSV file\n",
        "    generate_csv_file(output_file, all_weather_data)\n",
        "    print(f\"Weather data saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <span style=\"color: red;\">THESE CODES ARE TO CHECK ACCURACY OF YOUR CSV.</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The umber of unique cifes is:  55\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Weather_main</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Temp_min</th>\n",
              "      <th>Temp_max</th>\n",
              "      <th>Humidity</th>\n",
              "      <th>Clouds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>St. Helena</td>\n",
              "      <td>Jamestown</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>11:00</td>\n",
              "      <td>Snow</td>\n",
              "      <td>-10.84</td>\n",
              "      <td>-12.64</td>\n",
              "      <td>-10.84</td>\n",
              "      <td>89</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>St. Helena</td>\n",
              "      <td>Jamestown</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>14:00</td>\n",
              "      <td>Snow</td>\n",
              "      <td>-12.17</td>\n",
              "      <td>-13.29</td>\n",
              "      <td>-12.17</td>\n",
              "      <td>92</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>St. Helena</td>\n",
              "      <td>Jamestown</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>17:00</td>\n",
              "      <td>Snow</td>\n",
              "      <td>-12.78</td>\n",
              "      <td>-12.78</td>\n",
              "      <td>-12.78</td>\n",
              "      <td>92</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>St. Helena</td>\n",
              "      <td>Jamestown</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>20:00</td>\n",
              "      <td>Snow</td>\n",
              "      <td>-11.50</td>\n",
              "      <td>-11.50</td>\n",
              "      <td>-11.50</td>\n",
              "      <td>90</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>St. Helena</td>\n",
              "      <td>Jamestown</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>23:00</td>\n",
              "      <td>Snow</td>\n",
              "      <td>-13.16</td>\n",
              "      <td>-13.16</td>\n",
              "      <td>-13.16</td>\n",
              "      <td>89</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>South Sudan</td>\n",
              "      <td>Juba</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>11:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>32.59</td>\n",
              "      <td>32.59</td>\n",
              "      <td>37.50</td>\n",
              "      <td>26</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>South Sudan</td>\n",
              "      <td>Juba</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>14:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>33.92</td>\n",
              "      <td>33.92</td>\n",
              "      <td>35.81</td>\n",
              "      <td>24</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>South Sudan</td>\n",
              "      <td>Juba</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>17:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>36.55</td>\n",
              "      <td>36.55</td>\n",
              "      <td>36.55</td>\n",
              "      <td>17</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>South Sudan</td>\n",
              "      <td>Juba</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>20:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>29.12</td>\n",
              "      <td>29.12</td>\n",
              "      <td>29.12</td>\n",
              "      <td>29</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>South Sudan</td>\n",
              "      <td>Juba</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>23:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>26.76</td>\n",
              "      <td>26.76</td>\n",
              "      <td>26.76</td>\n",
              "      <td>34</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>275 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Country       City        Date   Time Weather_main   Temp  Temp_min  \\\n",
              "0     St. Helena  Jamestown  2025-01-20  11:00         Snow -10.84    -12.64   \n",
              "1     St. Helena  Jamestown  2025-01-20  14:00         Snow -12.17    -13.29   \n",
              "2     St. Helena  Jamestown  2025-01-20  17:00         Snow -12.78    -12.78   \n",
              "3     St. Helena  Jamestown  2025-01-20  20:00         Snow -11.50    -11.50   \n",
              "4     St. Helena  Jamestown  2025-01-20  23:00         Snow -13.16    -13.16   \n",
              "..           ...        ...         ...    ...          ...    ...       ...   \n",
              "270  South Sudan       Juba  2025-01-20  11:00       Clouds  32.59     32.59   \n",
              "271  South Sudan       Juba  2025-01-20  14:00       Clouds  33.92     33.92   \n",
              "272  South Sudan       Juba  2025-01-20  17:00       Clouds  36.55     36.55   \n",
              "273  South Sudan       Juba  2025-01-20  20:00       Clouds  29.12     29.12   \n",
              "274  South Sudan       Juba  2025-01-20  23:00       Clouds  26.76     26.76   \n",
              "\n",
              "     Temp_max  Humidity  Clouds  \n",
              "0      -10.84        89     100  \n",
              "1      -12.17        92     100  \n",
              "2      -12.78        92      84  \n",
              "3      -11.50        90      94  \n",
              "4      -13.16        89     100  \n",
              "..        ...       ...     ...  \n",
              "270     37.50        26      66  \n",
              "271     35.81        24      72  \n",
              "272     36.55        17      98  \n",
              "273     29.12        29      75  \n",
              "274     26.76        34      15  \n",
              "\n",
              "[275 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Weather_main</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Temp_min</th>\n",
              "      <th>Temp_max</th>\n",
              "      <th>Humidity</th>\n",
              "      <th>Clouds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Country, City, Date, Time, Weather_main, Temp, Temp_min, Temp_max, Humidity, Clouds]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Weather_main</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Temp_min</th>\n",
              "      <th>Temp_max</th>\n",
              "      <th>Humidity</th>\n",
              "      <th>Clouds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>11:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>21.98</td>\n",
              "      <td>21.98</td>\n",
              "      <td>27.36</td>\n",
              "      <td>65</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>14:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>21.93</td>\n",
              "      <td>21.93</td>\n",
              "      <td>23.25</td>\n",
              "      <td>61</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>17:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>19.96</td>\n",
              "      <td>19.96</td>\n",
              "      <td>19.96</td>\n",
              "      <td>70</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>20:00</td>\n",
              "      <td>Rain</td>\n",
              "      <td>17.39</td>\n",
              "      <td>17.39</td>\n",
              "      <td>17.39</td>\n",
              "      <td>85</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>23:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>15.15</td>\n",
              "      <td>15.15</td>\n",
              "      <td>15.15</td>\n",
              "      <td>92</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Country    City        Date   Time Weather_main   Temp  Temp_min  \\\n",
              "200  Rwanda  Kigali  2025-01-20  11:00       Clouds  21.98     21.98   \n",
              "201  Rwanda  Kigali  2025-01-20  14:00       Clouds  21.93     21.93   \n",
              "202  Rwanda  Kigali  2025-01-20  17:00       Clouds  19.96     19.96   \n",
              "203  Rwanda  Kigali  2025-01-20  20:00         Rain  17.39     17.39   \n",
              "204  Rwanda  Kigali  2025-01-20  23:00       Clouds  15.15     15.15   \n",
              "\n",
              "     Temp_max  Humidity  Clouds  \n",
              "200     27.36        65      59  \n",
              "201     23.25        61      79  \n",
              "202     19.96        70      99  \n",
              "203     17.39        85     100  \n",
              "204     15.15        92      85  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Weather_main</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Temp_min</th>\n",
              "      <th>Temp_max</th>\n",
              "      <th>Humidity</th>\n",
              "      <th>Clouds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>11:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>21.98</td>\n",
              "      <td>21.98</td>\n",
              "      <td>27.36</td>\n",
              "      <td>65</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>14:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>21.93</td>\n",
              "      <td>21.93</td>\n",
              "      <td>23.25</td>\n",
              "      <td>61</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>17:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>19.96</td>\n",
              "      <td>19.96</td>\n",
              "      <td>19.96</td>\n",
              "      <td>70</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>20:00</td>\n",
              "      <td>Rain</td>\n",
              "      <td>17.39</td>\n",
              "      <td>17.39</td>\n",
              "      <td>17.39</td>\n",
              "      <td>85</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>Rwanda</td>\n",
              "      <td>Kigali</td>\n",
              "      <td>2025-01-20</td>\n",
              "      <td>23:00</td>\n",
              "      <td>Clouds</td>\n",
              "      <td>15.15</td>\n",
              "      <td>15.15</td>\n",
              "      <td>15.15</td>\n",
              "      <td>92</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Country    City        Date   Time Weather_main   Temp  Temp_min  \\\n",
              "200  Rwanda  Kigali  2025-01-20  11:00       Clouds  21.98     21.98   \n",
              "201  Rwanda  Kigali  2025-01-20  14:00       Clouds  21.93     21.93   \n",
              "202  Rwanda  Kigali  2025-01-20  17:00       Clouds  19.96     19.96   \n",
              "203  Rwanda  Kigali  2025-01-20  20:00         Rain  17.39     17.39   \n",
              "204  Rwanda  Kigali  2025-01-20  23:00       Clouds  15.15     15.15   \n",
              "\n",
              "     Temp_max  Humidity  Clouds  \n",
              "200     27.36        65      59  \n",
              "201     23.25        61      79  \n",
              "202     19.96        70      99  \n",
              "203     17.39        85     100  \n",
              "204     15.15        92      85  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "csv = pd.read_csv('Vincent_African_Capitals_Weather.csv')\n",
        "\n",
        "print(\"The umber of unique cifes is: \", csv['City'].nunique())\n",
        "display(csv.dropna())\n",
        "\n",
        "display(csv.query('Country == \"US\"'))\n",
        "display(csv.query('Country == \"Rwanda\"'))\n",
        "display(csv.query('City == \"Kigali\"'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
